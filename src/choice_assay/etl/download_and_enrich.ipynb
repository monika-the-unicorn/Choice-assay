{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460a58aa",
   "metadata": {},
   "source": [
    "# Download and enrich CSV data\n",
    "\n",
    "This notebook downloads journal CSVs from blob storage using Expidite `cloud_connector`, aggregates them into one DataFrame, and labels behaviour.\n",
    "\n",
    "@@@ TODO: we should add a CSV file that contains the experimental data and then we can automatically enrich the raw data from the devices with this extra info to make analysis easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0a03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from expidite_rpi.core import configuration as root_cfg\n",
    "from expidite_rpi.core.cloud_connector import CloudConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8397af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading V3_CAPOSE files from 'expidite-journals' to C:\\Users\\bee-ops\\code\\Choice-assay\\src\\choice_assay\\etl\\downloads\n"
     ]
    }
   ],
   "source": [
    "# Required config\n",
    "AZURE_KEYS_FILE = Path.home() / \".expidite\" / \"choice_assay_keys.env\"\n",
    "\n",
    "CONTAINER_NAME = \"expidite-journals\"\n",
    "TYPE_ID = \"CAPOSE\"\n",
    "\n",
    "# Local download directory (relative to notebook working directory)\n",
    "DOWNLOAD_DIR = Path(\"./downloads\")\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREFIX = f\"V3_{TYPE_ID}\"\n",
    "SUFFIX = \".csv\"\n",
    "\n",
    "print(f\"Downloading {PREFIX} files from '{CONTAINER_NAME}' to {DOWNLOAD_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffaac41d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Keys file C:\\Users\\bee-ops\\.expidite\\choice_assay_keys.env does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m cc = CloudConnector.get_instance(root_cfg.CloudType.AZURE)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAZURE_KEYS_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m files = cc.list_cloud_files(CONTAINER_NAME, prefix=PREFIX, suffix=SUFFIX)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m matching CSV files in blobstore\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bee-ops\\code\\Choice-assay\\.venv\\Lib\\site-packages\\expidite_rpi\\core\\cloud_connector.py:85\u001b[39m, in \u001b[36mCloudConnector.set_keys\u001b[39m\u001b[34m(self, keys_file)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keys_file.exists():\n\u001b[32m     84\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKeys file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Create a new Keys class with the env_file set in the model_config\u001b[39;00m\n\u001b[32m     88\u001b[39m keys = root_cfg.Keys(_env_file=keys_file, _env_file_encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Keys file C:\\Users\\bee-ops\\.expidite\\choice_assay_keys.env does not exist"
     ]
    }
   ],
   "source": [
    "cc = CloudConnector.get_instance(root_cfg.CloudType.AZURE)\n",
    "cc.set_keys(AZURE_KEYS_FILE)\n",
    "files = cc.list_cloud_files(CONTAINER_NAME, prefix=PREFIX, suffix=SUFFIX)\n",
    "\n",
    "print(f\"Found {len(files)} matching CSV files in blobstore\")\n",
    "if files:\n",
    "    cc.download_container(src_container=CONTAINER_NAME,\n",
    "                          dst_dir=DOWNLOAD_DIR,\n",
    "                          files=files)\n",
    "    print(f\"Downloaded {len(files)} files to {DOWNLOAD_DIR.resolve()}\")\n",
    "else:\n",
    "    print(\"No matching files to download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaae811",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = sorted(DOWNLOAD_DIR.rglob(\"*.csv\"))\n",
    "print(f\"CSV files available locally: {len(csv_paths)}\")\n",
    "\n",
    "df_list = []\n",
    "for csv_path in csv_paths:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if not df.empty:\n",
    "        df[\"source_file\"] = csv_path.name\n",
    "        df_list.append(df)\n",
    "\n",
    "if df_list:\n",
    "    aggregated_df = pd.concat(df_list, ignore_index=True)\n",
    "else:\n",
    "    aggregated_df = pd.DataFrame()\n",
    "\n",
    "print(f\"Aggregated rows: {len(aggregated_df)}\")\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29140ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation before behaviour classification\n",
    "required_columns = ['Tube_prob_likelihood', 'End_prob_likelihood']\n",
    "missing_columns = [col for col in required_columns if col not in aggregated_df.columns]\n",
    "\n",
    "if aggregated_df.empty:\n",
    "    print('Validation: aggregated_df is empty.')\n",
    "elif missing_columns:\n",
    "    raise KeyError(f\"Validation failed. Missing required columns: {missing_columns}\")\n",
    "else:\n",
    "    for col in required_columns:\n",
    "        aggregated_df[col] = pd.to_numeric(aggregated_df[col], errors='coerce')\n",
    "\n",
    "    invalid_rows = aggregated_df[required_columns].isna().any(axis=1).sum()\n",
    "    print(f'Validation: {len(aggregated_df)} total rows')\n",
    "    print(f'Validation: {invalid_rows} rows have invalid/missing likelihood values')\n",
    "\n",
    "    if invalid_rows:\n",
    "        display(aggregated_df.loc[aggregated_df[required_columns].isna().any(axis=1), required_columns + ['source_file']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to classify behavior\n",
    "def get_behaviour(row):\n",
    "    behaviour = 'No_prob'\n",
    "    if (row['Tube_prob_likelihood'] >= 0.6) & (row['End_prob_likelihood'] >= 0.6):\n",
    "        behaviour = \"Drinking\"\n",
    "    elif (row['Tube_prob_likelihood'] >= 0.6) ^ (row['End_prob_likelihood'] >= 0.6):\n",
    "        behaviour = \"Prob_out\"\n",
    "    return behaviour\n",
    "\n",
    "required_columns = ['Tube_prob_likelihood', 'End_prob_likelihood']\n",
    "missing_columns = [col for col in required_columns if col not in aggregated_df.columns]\n",
    "\n",
    "if aggregated_df.empty:\n",
    "    print('No data loaded; behaviour classification skipped.')\n",
    "elif missing_columns:\n",
    "    raise KeyError(f'Missing required columns for behaviour classification: {missing_columns}')\n",
    "else:\n",
    "    before_count = len(aggregated_df)\n",
    "    clean_df = aggregated_df.dropna(subset=required_columns).copy()\n",
    "    dropped_count = before_count - len(clean_df)\n",
    "\n",
    "    if dropped_count:\n",
    "        print(f'Dropped {dropped_count} rows with invalid/missing likelihood values before classification.')\n",
    "\n",
    "    clean_df['Behaviour'] = clean_df.apply(get_behaviour, axis=1)\n",
    "    aggregated_df = clean_df\n",
    "    print(aggregated_df['Behaviour'].value_counts(dropna=False))\n",
    "\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c52d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = DOWNLOAD_DIR / 'aggregated_journals_with_behaviour.csv'\n",
    "if not aggregated_df.empty:\n",
    "    aggregated_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved aggregated dataset to: {output_path.resolve()}\")\n",
    "else:\n",
    "    print('Aggregated dataframe is empty; no output written.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
